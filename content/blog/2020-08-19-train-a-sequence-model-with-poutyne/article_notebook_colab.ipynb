{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> In this article, we will train an RNN, or more precisely, an LSTM, to predict the sequence of tags associated with a\n",
    "given address, known as address parsing.\n",
    "\n",
    "> Also, the article is available in a [Jupyter Notebook](https://github.com/dot-layer/blog/blob/master/Github/blog/content/blog/2020-08-19-train-a-sequence-model-with-poutyne/article_notebook.ipynb) or in a [Google Colab Jupyter notebook](https://colab.research.google.com/github/dot-layer/blog/blob/post%2Fdb_sequence_training_poutyne/content/blog/2020-08-19-train-a-sequence-model-with-poutyne/article_notebook_colab.ipynb).\n",
    ">\n",
    "> Before starting this article, we would like to disclaim that this tutorial is greatly inspired by an online tutorial David created for the Poutyne framework. Also, the content is based on a recent [article](https://arxiv.org/abs/2006.16152) we wrote about address tagging. However, there are differences between the present work and the two others, as this one is specifically designed for the less technical reader.\n",
    "\n",
    "\n",
    "Sequential data, such as addresses, are pieces of information that are deliberately given in a specific order. In other words, they are sequences with a particular structure; and knowing this structure is crucial for predicting the missing entries of a given truncated sequence. For example,\n",
    "when writing an address, we know, in Canada, that after the civic number (e.g. 420), we have the street name (e.g. du Lac).\n",
    "Hence, if one is asked to complete an address containing only a number, he can reasonably assume that the next information that should be added to the sequence is a street name. Various modelling approaches have been proposed to make predictions over sequential data. Still, more recently, deep learning models known as Recurrent Neural Network (RNN) have been introduced for this type of data.\n",
    "\n",
    "The main purpose of this article is to introduce the various tricks (e.g., padding and packing) that are required for training an RNN. Before we do that, let us define our \"address\" problem more formally and elaborate on what RNNs (and LSTMs) actually are.\n",
    "\n",
    "## Address Tagging\n",
    "Address tagging is the task of detecting and tagging the different parts of an address such as the civic number,\n",
    "the street name or the postal code (or zip code). The following figure shows an example of such a tagging.\n",
    "\n",
    "![address parsing canada](address_parsing.png)\n",
    "\n",
    "For our purpose, we define 8 pertinent tags that can be found in an address: `[StreetNumber, StreetName, Orientation, Unit, Municipality, Province, PostalCode, GeneralDelivery]`.\n",
    "\n",
    "Since addresses are sequences of arbitrary length where a word's index does not mean as much as its position relative to others, one can hardly rely on a simple fully connected neural network for address tagging.\n",
    "A dedicated type of neural networks was specifically designed for this kind of tasks involving sequential data: RNNs.\n",
    "\n",
    "## Recurrent Neural Network (RNN)\n",
    "\n",
    "In brief, an RNN is a neural network in which connections between nodes form a temporal sequence. It means that this type of network\n",
    "allows previous outputs to be used as inputs for the next prediction.\n",
    "For more information regarding RNNs, have a look at Stanford's freely available [cheastsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks).\n",
    "\n",
    "For our purpose, we do not use the vanilla RNN, but a widely-use variant of it known as long short-term memory (LSTM) network. This latter, which involves components called gates, is often preferred over its competitors due to its better stability with respect to gradient update (vanishing and exploding gradient).\n",
    "To learn more about LSTMs, see [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for an in-depth explanation.\n",
    "\n",
    "For now, let's simply use a single layer unidirectional LSTM. We will, later on, explore the use of more layers and a bidirectional approach.\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Since our data is text, we will use a well-known text encoding technique: word embeddings. Word embeddings are vector\n",
    "representations of words. The main hypothesis underlying their use is that there exists a linear relation between words. For example, the linear relation\n",
    "between the word `king` and `queen` is gender. So logically, if we remove the vector corresponding to `male` to the one for `king`, and then add the vector for\n",
    "`female`, we should obtain the vector corresponding to `queen` (i.e. `king - male + female = queen`). That being said, this kind of representation is usually made in high dimensions such as `300`, which\n",
    "makes it impossible for humans to reason about them. Neural networks, on the other hand, can efficiently make use of the implicit relations despite their high dimensionality.\n",
    "\n",
    "We therefore fix our LSTM's input and hidden state dimensions to the same sizes as the vectors of embedded words.\n",
    "For the present purpose, we will use the\n",
    "[French pre-trained](https://fasttext.cc/docs/en/crawl-vectors.html) fastText embeddings of dimension `300`.\n",
    "\n",
    "### The PyTorch Model\n",
    "\n",
    "Let us first import all the necessary packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install --upgrade poutyne #install poutyne\n",
    "%pip install --upgrade colorama #install colorama\n",
    "%pip install --upgrade pymagnitude-light #install pymagnitude-light\n",
    "%matplotlib inline\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from poutyne import set_seeds\n",
    "from poutyne.framework import Experiment\n",
    "from pymagnitudelight import Magnitude\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's create a single (i.e. one layer) unidirectional LSTM with `input_size` and `hidden_size` of `300`. We\n",
    "will explore later on the effect of stacking more layers and using a bidirectional approach.\n",
    "\n",
    "> See [here](https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402) why we use the `batch_first` argument."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dimension = 300\n",
    "num_layer = 1\n",
    "bidirectional = False\n",
    "\n",
    "lstm_network = nn.LSTM(input_size=dimension,\n",
    "                       hidden_size=dimension,\n",
    "                       num_layers=num_layer,\n",
    "                       bidirectional=bidirectional,\n",
    "                       batch_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully-connected Layer\n",
    "Since the output of the LSTM network is of dimension `300`, we will use a fully-connected layer to map it into\n",
    "a space of equal dimension to that of the tag space (i.e. number of tags to predict), that is 8.\n",
    "Finally, since we want to predict the most probable tokens, we will apply the softmax function on this layer\n",
    "(see [here](https://en.wikipedia.org/wiki/Softmax_function) if softmax does not ring a bell).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_dim = dimension #the output of the LSTM\n",
    "tag_dimension = 8\n",
    "\n",
    "fully_connected_network = nn.Linear(input_dim, tag_dimension)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Constants\n",
    "\n",
    "Now, let's set our training constants. We first specify a CUDA (GPU) device for training (using a CPU takes way too long,\n",
    "if you don't have one, you can use the Google Colab notebook).\n",
    "\n",
    "Second, we set the batch size (i.e. the number of elements to see before updating the model), the learning rate for the optimizer\n",
    "and the number of epochs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "\n",
    "epoch_number = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need to set Pythons's, NumPy's and PyTorch's random seeds using the Poutyne function to make our training (almost) completely reproducible.\n",
    "\n",
    "> See [here](https://determined.ai/blog/reproducibility-in-ml/) for an explanation on why setting seed does not guarantee complete reproducibility."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Dataset\n",
    "The dataset consists of `1,010,987` complete French and English Canadian addresses and their associated tags.\n",
    "Here's an example address\n",
    "\n",
    "`\"420 rue des Lilas Ouest, Québec, G1V 2V3\"`\n",
    "\n",
    "and its corresponding tags\n",
    "\n",
    "`[StreetNumber, StreetName, StreetName, StreetName, Orientation, Municipality, PostalCode, PostalCode]`.\n",
    "\n",
    "Now let's download our dataset. For simplicity, a `100,000` addresses test set is kept aside, with 80% of the remaining addresses used for training and 20 % used as a validation set.\n",
    "Also note that the dataset was pickled for simplicity (using a Python `list`). Here is the code to download it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_data(saving_dir, data_type):\n",
    "    \"\"\"\n",
    "    Function to download the dataset using data_type to specify if we want the train, valid or test.\n",
    "    \"\"\"\n",
    "\n",
    "    # hardcoded url to download the pickled dataset\n",
    "    root_url = \"https://dot-layer.github.io/blog-external-assets/train_rnn/{}.p\"\n",
    "\n",
    "    url = root_url.format(data_type)\n",
    "    r = requests.get(url)\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    open(os.path.join(saving_dir, f\"{data_type}.p\"), 'wb').write(r.content)\n",
    "\n",
    "\n",
    "download_data('./data/', \"train\")\n",
    "download_data('./data/', \"valid\")\n",
    "download_data('./data/', \"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's load the data in memory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data = pickle.load(open(\"./data/train.p\", \"rb\"))  # 728,789 examples\n",
    "valid_data = pickle.load(open(\"./data/valid.p\", \"rb\"))  # 182,198 examples\n",
    "test_data = pickle.load(open(\"./data/test.p\", \"rb\"))  # 100,000 examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As explained before, the (train) dataset is a list of `728,789` tuples where the first element is the full address, and the second is a list of tags (the ground truth)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[:2] # The first two train items"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> (the output)\n",
    "\n",
    "![data_snapshot](data_snapshot.png)\n",
    "\n",
    "### Vectorize the Dataset\n",
    "\n",
    "Since we used word embeddings as the encoded representations of the words in the addresses, we need to *convert* the addresses into the corresponding word vectors. In order to do that, we will use a `vectorizer` (i.e. the process of converting words into vectors). This embedding vectorizer will extract, for each word, the embedding value based on the pre-trained French fastText model. We use French embeddings because French is the language in which most of the adresses in our dataset are written.\n",
    "\n",
    "> About Magnitude fastText model\n",
    "> Since the original fastText model take a lot of RAM (~9 GO). I've come across magnitude when I've published a model, and the model was so large with the embedding that it could not fit in a typical computer.\n",
    "> The idea behind Magnitude is to convert the original vectors into a mapping between the word and subword and the vectors using a local database.\n",
    "> The conversion took about 8 hours to do, and the script is broken for fastText embeddings. It would be a little bit slower, but Google Colab doesn't allow us to use more than 12 GO.\n",
    "> But a drawback of Magnitude is that for a reason I don't understand, I can't make it work in multithreading on three different computers, but it works on Colab, even if the doc says it should work easily."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_from_url(model: str, saving_dir: str, extension: str):\n",
    "    \"\"\"\n",
    "    Simple function to download the content of a file from a distant repository.\n",
    "    \"\"\"\n",
    "    print(\"Downloading the model.\")\n",
    "    model_url = \"https://graal.ift.ulaval.ca/public/deepparse/{}.\" + extension\n",
    "    url = model_url.format(model)\n",
    "    r = requests.get(url)\n",
    "\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "    open(os.path.join(saving_dir, f\"{model}.{extension}\"), \"wb\").write(r.content)\n",
    "\n",
    "\n",
    "def download_fasttext_magnitude_embeddings(saving_dir):\n",
    "    \"\"\"\n",
    "    Function to download the magnitude pre-trained fastText model.\n",
    "    \"\"\"\n",
    "    model = \"fasttext\"\n",
    "    extension = \"magnitude\"\n",
    "    file_name = os.path.join(saving_dir, f\"{model}.{extension}\")\n",
    "    if not os.path.isfile(file_name):\n",
    "        warnings.warn(\"The fastText pre-trained word embeddings will be download in magnitude format (2.3 GO), \"\n",
    "                      \"this process will take several minutes.\")\n",
    "        extension = extension + \".gz\"\n",
    "        download_from_url(model=model, saving_dir=saving_dir, extension=extension)\n",
    "        gz_file_name = file_name + \".gz\"\n",
    "        print(\"Unzip the model.\")\n",
    "        with gzip.open(os.path.join(saving_dir, gz_file_name), \"rb\") as f:\n",
    "            with open(os.path.join(saving_dir, file_name), \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f, f_out)\n",
    "        os.remove(os.path.join(saving_dir, gz_file_name))\n",
    "    return file_name\n",
    "\n",
    "\n",
    "class EmbeddingVectorizer:\n",
    "    def __init__(self, path=\"./\"):\n",
    "        \"\"\"\n",
    "        Embedding vectorizer\n",
    "        \"\"\"\n",
    "        file_name = download_fasttext_magnitude_embeddings(saving_dir=path)\n",
    "        self.embedding_model = Magnitude(file_name)\n",
    "\n",
    "    def __call__(self, address):\n",
    "        \"\"\"\n",
    "        Convert address to embedding vectors\n",
    "        :param address: The address to convert\n",
    "        :return: The embeddings vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for word in address.split():\n",
    "            embeddings.append(self.embedding_model.query(word))\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "embedding_vectorizer = EmbeddingVectorizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need to apply a similar operation to the address tags (e.g. StreetNumber, StreetName).\n",
    "This time, however, the `vectorizer` needs to convert the tags into categorical values (e.g. StreetNumber -> 0).\n",
    "For simplicity, we will use a `DatasetBucket` class that will apply the vectorizing process using both\n",
    "the embedding and the address vectorization process that we've just described during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DatasetBucket:\n",
    "    def __init__(self, data, embedding_vectorizer):\n",
    "        self.data = data\n",
    "        self.embedding_vectorizer = embedding_vectorizer\n",
    "        self.tags_set = {\n",
    "            \"StreetNumber\": 0,\n",
    "            \"StreetName\": 1,\n",
    "            \"Unit\": 2,\n",
    "            \"Municipality\": 3,\n",
    "            \"Province\": 4,\n",
    "            \"PostalCode\": 5,\n",
    "            \"Orientation\": 6,\n",
    "            \"GeneralDelivery\": 7\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):  # We vectorize when data is asked\n",
    "        data = self.data[item]\n",
    "        return self._item_vectorizing(data)\n",
    "\n",
    "    def _item_vectorizing(self, item):\n",
    "        address = item[0]\n",
    "        address_vector = self.embedding_vectorizer(address)\n",
    "\n",
    "        tags = item[1]\n",
    "        idx_tags = self._convert_tags_to_idx(tags)\n",
    "\n",
    "        return address_vector, idx_tags\n",
    "\n",
    "    def _convert_tags_to_idx(self, tags):\n",
    "        idx_tags = []\n",
    "        for tag in tags:\n",
    "            idx_tags.append(self.tags_set[tag])\n",
    "        return idx_tags\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_vectorizer = DatasetBucket(train_data, embedding_vectorizer)\n",
    "valid_dataset_vectorizer = DatasetBucket(valid_data, embedding_vectorizer)\n",
    "test_dataset_vectorizer = DatasetBucket(test_data, embedding_vectorizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Here is a example of the vectorizing process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "address, tag = train_dataset_vectorizer[0] # Unpack the first tuple\n",
    "print(f\"The vectorized address is now a list of vectors {address}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Tag is now a list of integers : {tag}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### DataLoader\n",
    "> We use a first trick, ``padding``.\n",
    "\n",
    "Now, because the addresses are not all of the same size, it is impossible to batch them together; recall that all tensor elements must have the same lengths. But there is a trick: padding!\n",
    "\n",
    "The idea is simple; we add *empty* tokens at the end of each sequence until they reach the length of the longest one in the batch. For example, if we have three sequences of length ${1, 3, 5}$, padding will add 4 and 2 *empty* tokens respectively to the first two.\n",
    "\n",
    "For the word vectors, we add vectors of 0 as padding. For the tag indices, we pad with -100's. We do so because the [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) and the accuracy metric both ignore targets with values of -100.\n",
    "\n",
    "To do the padding, we use the `collate_fn` argument of the [PyTorch `DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), and on running time, the process will be done by the `DataLoader`. One thing to keep in mind when treating padded sequences is that their original length will be required to unpad them later on in the forward pass. That way, we can pad and pack the sequence to minimize the training time (read [this good explanation](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) on why we pack sequences)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def pad_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    The collate_fn that can add padding to the sequences so all can have\n",
    "    the same length as the longest one.\n",
    "\n",
    "    Args:\n",
    "        batch (List[List, List]): The batch data, where the first element\n",
    "        of the tuple is the word idx and the second element are the target\n",
    "        label.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (x, y). The element x is a tuple containing (1) a tensor of padded\n",
    "        word vectors and (2) their respective original sequence lengths. The element\n",
    "        y is a tensor of padded tag indices. The word vectors are padded with vectors\n",
    "        of 0s and the tag indices are padded with -100s. Padding with -100 is done\n",
    "        because of the cross-entropy loss and the accuracy metric ignores\n",
    "        the targets with values -100.\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets us two lists of tensors and a list of integer.\n",
    "    # Each tensor in the first list is a sequence of word vectors.\n",
    "    # Each tensor in the second list is a sequence of tag indices.\n",
    "    # The list of integer consist of the lengths of the sequences in order.\n",
    "    sequences_vectors, sequences_labels, lengths = zip(*[\n",
    "        (torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n",
    "        for (seq_vectors, labels) in sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    ])\n",
    "\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    padded_sequences_vectors = pad_sequence(sequences_vectors, batch_first=True, padding_value=0)\n",
    "\n",
    "    padded_sequences_labels = pad_sequence(sequences_labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return (padded_sequences_vectors, lengths), padded_sequences_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset_vectorizer, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset_vectorizer, batch_size=batch_size, collate_fn=pad_collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset_vectorizer, batch_size=batch_size, collate_fn=pad_collate_fn, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Network\n",
    "> We use a second trick, ``packing``.\n",
    "\n",
    "Since our sequences are of variable lengths and that we want to be as efficient as possible when packing them, we cannot use the [PyTorch `nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) class to define our model. Instead, we define the forward pass so that it uses packed sequences (again, you can read [this good explanation](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) on why we pack sequences)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RecurrentNet(nn.Module):\n",
    "    def __init__(self, lstm_network, fully_connected_network):\n",
    "        super().__init__()\n",
    "        self.hidden_state = None\n",
    "\n",
    "        self.lstm_network = lstm_network\n",
    "        self.fully_connected_network = fully_connected_network\n",
    "\n",
    "    def forward(self, padded_sequences_vectors, lengths):\n",
    "        \"\"\"\n",
    "            Defines the computation performed at every call.\n",
    "\n",
    "            Shapes:\n",
    "                padded_sequences_vectors: batch_size * longest_sequence_length (padding), 300\n",
    "                lengths: batch_size\n",
    "\n",
    "        \"\"\"\n",
    "        total_length = padded_sequences_vectors.shape[1]\n",
    "        pack_padded_sequences_vectors = pack_padded_sequence(padded_sequences_vectors, lengths.cpu(), batch_first=True)\n",
    "\n",
    "        lstm_out, self.hidden_state = self.lstm_network(pack_padded_sequences_vectors)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True, total_length=total_length)\n",
    "\n",
    "        tag_space = self.fully_connected_network(lstm_out) # shape: batch_size * longest_sequence_length, 8 (tag space)\n",
    "        return tag_space.transpose(-1, 1) # we need to transpose since it's a sequence # shape: batch_size * 8, longest_sequence_length\n",
    "\n",
    "full_network = RecurrentNet(lstm_network, fully_connected_network)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "We have created an LSTM network (`lstm_network`) and a fully connected network (`fully_connected_network`), and we use both\n",
    "components in the full network. The full network makes use of padded-packed sequences,\n",
    "so we created the `pad_collate_fn` function to do the necessary work within the `DataLoader`. Finally,\n",
    "we will load the data using the vectorizer (within the `DataLoader` using the `pad_collate` function). This means that the addresses will be represented by word embeddings.\n",
    "Also, the address components will be converted into categorical value (from 0 to 7).\n",
    "\n",
    "\n",
    "## The Training\n",
    "\n",
    "Now that we have all the components for the network, let's define our optimizer (Stochastic Gradient Descent)\n",
    "([SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(full_network.parameters(), lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Poutyne Experiment\n",
    "> Disclaimer: David is a developer on the Poutyne library, so we will present code using this framework. See the project [here](https://poutyne.org/).\n",
    "\n",
    "Let's create our experiment using Poutyne for automated logging in the project root directory (`./`). We will also set\n",
    "the loss function and a batch metric (accuracy) to monitor the training. The accuracy is computed on the word-tag level, meaning that every correct tag prediction is a good prediction. For example, the accuracy of the prediction `StreetNumber, StreetName` with the ground truth `StreetNumber, StreetName` is 1 and the accuracy of the prediction `StreetNumber, StreetNumber` with the ground truth `StreetNumber, StreetName` is 0.5."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp = Experiment(\"./\", full_network, device=device, optimizer=optimizer,\n",
    "                 loss_function=cross_entropy, batch_metrics=[\"acc\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using our experiment, we can now launch the training as simply as"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.train(train_loader, valid_generator=valid_loader, epochs=epoch_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It will take around 40 minutes per epochs, so a couple hours for the complete training.\n",
    "\n",
    "### Results\n",
    "The next figure shows the loss and the accuracy during our training (blue) and during our validation (orange) steps.\n",
    "After 10 epochs, we obtain a validation loss and accuracy of `0.01981` and `99.54701` respectively, satisfying values for a first model. Also, since our training accuracy and loss closely match their respective validation values, our model does not appear to be overfitted on the training set.\n",
    "\n",
    "![loss_acc](graph/training_graph.png)\n",
    "\n",
    "## Bigger model\n",
    "\n",
    "It seems that our model performed pretty well, but just for fun, let's unleash the full potential of LSTMs using a\n",
    "bidirectional approach (bidirectional LSTM). What it means is that instead of _simply_ viewing the sequence from the start to the end, we\n",
    "also train the model to see the sequence from the end to the start. It's important to state that the two directions are\n",
    "not shared, meaning that the model _sees_ the sequence in one direction at the time, but gathers the information from both directions into the\n",
    "fully connected layer. That way, our model can get insight from both directions.\n",
    "\n",
    "Instead of using only one layer, let's use a bidirectional bi-LSTM, which means that we use two layers of hidden state for each direction.\n",
    "\n",
    "So, let's create the new LSTM and fully connected network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dimension = 300\n",
    "num_layer = 2\n",
    "bidirectional = True\n",
    "\n",
    "lstm_network = nn.LSTM(input_size=dimension,\n",
    "                       hidden_size=dimension,\n",
    "                       num_layers=num_layer,\n",
    "                       bidirectional=bidirectional,\n",
    "                       batch_first=True)\n",
    "\n",
    "input_dim = dimension * 2 #since bidirectional\n",
    "\n",
    "fully_connected_network = nn.Linear(input_dim, tag_dimension)\n",
    "\n",
    "full_network_bi_lstm = RecurrentNet(lstm_network, fully_connected_network)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_bi_lstm = Experiment(\"./\", full_network_bi_lstm, device=device, optimizer=optimizer,\n",
    "                         loss_function=cross_entropy, batch_metrics=[\"acc\"])\n",
    "exp_bi_lstm.train(train_loader, valid_generator=valid_loader, epochs=epoch_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "Here are our validation results for the last epoch of the larger model. On the validation dataset,\n",
    "we can see that we obtain a marginal gain of around `0.3` % for the accuracy over our previous simpler model. This is only a slight improvement.\n",
    "\n",
    "|   Model  | Bidirectional bi-LSTM |\n",
    "|:--------:|:------------------:|\n",
    "|   Loss   |    0.0050          |\n",
    "| Accuracy |    99.8594         |\n",
    "\n",
    "But now that we have our two trained models, let's use the test set as a final and **unique** step for evaluating their performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.test(test_loader)\n",
    "exp_bi_lstm.test(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next table presents the results of the bidirectional bi-LSTM with two layers and the previous model (LSTM with one layer).\n",
    "\n",
    "|   Model  | LSTM one layer | Bidirectional bi-LSTM |\n",
    "|:--------:|:--------------:|:------------------:|\n",
    "|   Loss   |     0.0152     |    **0.0050**      |\n",
    "| Accuracy |     99.5758    |    **99.8550**     |\n",
    "\n",
    "We see similar validation results for both models. Also, we still see a little improvement in accuracy and total loss for the larger model. Considering that we only improved by around `0.3` %, one can\n",
    "argue that the difference is only due to training variance (mostly due to our random sampling of training batches). To test the robustness of our approach, we could train our model multiple times\n",
    "using different random seeds and report the mean and standard deviation of each metric over all experiments rather than the result of a single training. Let's try something else.\n",
    "\n",
    "### Zero Shot Evaluation\n",
    "Since we have at our disposition addresses from other countries, let's see if our model has really learned a typical address sequence\n",
    "or if it has simply *memorized* all the training examples.\n",
    "\n",
    "We will test our model on three different types of dataset\n",
    "\n",
    "   - first, on addresses with the exact same structure\n",
    "    as in our training dataset: addresses from the United States of America (US) and the United Kingdom (UK)\n",
    "   - secondly, on addresses with the exact same structure as those in our training dataset **but**\n",
    "    written in a totally different language: addresses from Russia (RU)\n",
    "   - finally, on addresses that exhibit a different structure **and** that are written in a different language: addresses from Mexico (MX).\n",
    "\n",
    "For each test, we will use a dataset of `100,000` examples in total, and we will evaluate using the best epoch of our two models (i.e. last epoch for both of them).\n",
    "Also, we will use the same pre-processing steps as before (i.e. data vectorization, the same pad collate function), but we will only apply\n",
    "a test phase, meaning no training step.\n",
    "\n",
    "First, let's download and vectorize all the needed datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "download_data('./data/', \"us\")\n",
    "download_data('./data/', \"gb\")\n",
    "download_data('./data/', \"ru\")\n",
    "download_data('./data/', \"mx\")\n",
    "\n",
    "us_data = pickle.load(open(\"./data/us.p\", \"rb\"))  # 100,000 examples\n",
    "gb_data = pickle.load(open(\"./data/gb.p\", \"rb\"))  # 100,000 examples\n",
    "ru_data = pickle.load(open(\"./data/ru.p\", \"rb\"))  # 100,000 examples\n",
    "mx_data = pickle.load(open(\"./data/mx.p\", \"rb\"))  # 100,000 examples\n",
    "\n",
    "dataset_vectorizer.vectorize(us_data)\n",
    "dataset_vectorizer.vectorize(gb_data)\n",
    "dataset_vectorizer.vectorize(ru_data)\n",
    "dataset_vectorizer.vectorize(mx_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### First Test\n",
    "\n",
    "Now let's test for the United States of America and United Kingdom."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "us_loader = DataLoader(us_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(us_loader)\n",
    "exp_bi_lstm.test(us_loader)\n",
    "\n",
    "gb_loader = DataLoader(gb_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(gb_loader)\n",
    "exp_bi_lstm.test(gb_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next table presents the results of both models for both countries. We obtain\n",
    "better results for the two countries using the bidirectional bi-LSTM (around 8% better). It's interesting to see that, considering address structures are similar to those in the training dataset (Canada), we obtain near as good results as those observed during training. This suggests that our model seems to have learned to recognize the structure of an address. Also, despite the language being the same as in the training dataset (i.e. some English addresses in the bilingual canadian address dataset), we obtain poorer results. That situation is most likely due to the fact that the postal code formats are not the same. For the US, it is 5 digits, and for the UK it is similar to that of Canada, but it is not always a letter followed by a number and not always 6 characters. It is *normal* for a model to\n",
    "have difficulty when faced with new patterns. All in all, we can say that our model has achieved good results.\n",
    "\n",
    "| Model (Country) | LSTM one layer | Bidirectional bi-LSTM |\n",
    "|:---------------:|:--------------:|:------------------:|\n",
    "|    Loss (US)    |     0.6176     |       **0.3078**   |\n",
    "|  Accuracy (US)  |     84.7396    |       **91.8220**  |\n",
    "|    Loss (UK)    |     0.4368     |       **0.1571**   |\n",
    "|  Accuracy (UK)  |     86.2543    |       **95.6840**  |\n",
    "\n",
    "\n",
    "##### The Second and Third Test\n",
    "\n",
    "Now let's test for Russia and Mexico.\n",
    "\n",
    "But first, let's discuss how our French embeddings can generate word vectors for vocabulary in a different language. FastText uses subword embeddings when complete embeddings do not exist. For example, we can assume the presence of a word embedding vector for the word `Roi`, but we face an out-of-vocabulary (OOV) for the word `H1A1` since this word is not a real word. The trick with fastText is that it creates composite embeddings using the subword with fixed window size (length of the subword) when facing OOV words. For example, a two characters window embeddings of `H1A1` would be the aggregated embeddings of the subword `H1`, `1A` and `A1`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ru_loader = DataLoader(ru_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(ru_loader)\n",
    "exp_bi_lstm.test(ru_loader)\n",
    "\n",
    "mx_loader = DataLoader(mx_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(mx_loader)\n",
    "exp_bi_lstm.test(mx_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next table presents the results of both models for the two countries tested. We see that the first test\n",
    "(RU) gives poorer results than those for Mexican addresses, even if these latter are written in a different structure and language. This situation could be explained by both languages' roots; Spanish is closer to French than Russian is.\n",
    "An interesting thing is that even in a *difficult* annotation context, both models perform relatively well.\n",
    "It suggests that our models have really learned the *logic* of an address sequence. It could also mean that, if \n",
    "we train our model longer, we could potentially improve our results. Other modifications that could improve our models are discussed in the next and final section.\n",
    "\n",
    "| Model (Country) | LSTM one layer | Bidirectional bi-LSTM |\n",
    "|:---------------:|:--------------:|:------------------:|\n",
    "|    Loss (RU)    |   **2.5181**   |       4.6118       |\n",
    "|  Accuracy (RU)  |   **48.9820**  |       47.3185      |\n",
    "|    Loss (MX)    |     2.6786     |     **1.7147**     |\n",
    "|  Accuracy (MX)  |     50.2013    |     **63.5317**    |\n",
    "\n",
    "### Summary\n",
    "In summary, we found that using a bidirectional bi-LSTM seems to perform better on addresses not seen during training, including those coming from other countries. Still, the results for addresses from other countries are not as good as those for Canadian addresses (training dataset). A solution to this problem could be to train a model using all the\n",
    "data from all over the world. This approach was used by [Libpostal](https://github.com/openvenues/libpostal), which trained a \n",
    "CRF over an impressive near `100` million addresses (yes, **100 million**). If you want to explore this avenue, the data they used is publicly available [here](https://github.com/openvenues/libpostal).\n",
    "\n",
    "We also explored the idea that the language disparity has a negative impact on the results, since we use monolingual word embeddings (i.e. French), which is *normal* considering that they were trained for a specific language.\n",
    "\n",
    "> Alert of self-promotion of our work here\n",
    "We've personally explored this avenue in an article using [subword embedding for address parsing](https://arxiv.org/abs/2006.16152).  \n",
    "\n",
    "That being said, our model still performed well on the Canadian dataset, and one can simply train simpler LSTM model using\n",
    "country data to obtain the best results possible with a model as simple as possible. "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}