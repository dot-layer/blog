{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade poutyne #install poutyne on colab\n",
    "%pip install --upgrade colorama #install colorama on colab\n",
    "%matplotlib inline\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from io import TextIOBase\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from poutyne import set_seeds\n",
    "from poutyne.framework import Experiment\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this article, we will train an RNN, or more precisely, an LSTM, to predict the sequence of tags associated with a \n",
    "given address, known as parsing address. \n",
    "\n",
    "> Also, the code (only) is available in [this Google Colab Jupyter notebook](https://colab.research.google.com/github/dot-layer/blog/blob/post%2Fdb_sequence_training_poutyne/content/blog/2020-08-19-train-a-sequence-model-with-poutyne/article_notebook.ipynb#scrollTo=c1JxpCRsQN0e).\n",
    "\n",
    "> Before starting this article, I would like to disclaim that this tutorial is greatly inspired by a online tutorial I \n",
    "created for Poutyne framework. Also, the content is based on a recent article I wrote with Marouane Yassine. However, \n",
    "there are differences between the present work and the two others, I've tried to put more insights for the less technical reader in this one.\n",
    "\n",
    "\n",
    "Sequential data, such as an address, are data that we assume are conditional to the previously known information. For example,\n",
    "when writing an address, we know, in Canada, that after the civic number (e.g. 420), we have the street name (e.g. du Lac).\n",
    "That means that if we know the sequence's structure, we can predict the following information. Various modelling approaches have been proposed to make predictions over sequential data. Still, more recently, deep learning models known as Recurrent Neural Network (RNN) has been introduced for this type of data.\n",
    "\n",
    "Training an RNN requires various tricks (padding and packing) that we will explore in this article. First, let's\n",
    "state our problem, and later on, we will discuss what an actual RNN or LSTM is.\n",
    "\n",
    "## Address Tagging\n",
    "Address tagging is the task of detecting, by tagging, the different parts of an address such as the civic number,\n",
    "the street name or the postal code (or zip code). The following figure shows an example of such a tagging.\n",
    "\n",
    "![address parsing canada](address_parsing.png)\n",
    "\n",
    "Since addresses are written in a predetermined sequence, RNN is the best way to crack this problem. But to decode the output of the RNN, we also need another component, a fully-connected layer.\n",
    "So, our architecture, we will be composed of an RNN and a fully-connected layer.\n",
    "\n",
    "## RNN\n",
    "Speaking of RNN, what is it?\n",
    "\n",
    "In brief, RNN is neural networks where connections between nodes form a temporal sequence. It means that this type of networks\n",
    "allow previous outputs to be used as inputs of the previous information\n",
    "([For more about RNN](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)).\n",
    "\n",
    "But, instead of using a vanilla RNN, we use a variant of it, know as a long short-term memory (LSTM) that is known to have\n",
    "better stability with gradient update (vanishing and exploding gradient) by using gates\n",
    "([to learn more about LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)).\n",
    "\n",
    "Also, for now, let's simply use a single layer unidirectional LSTM. We will, later on, explore the use of more layers and bidirectional approach.\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Since our data is text, we will use a well-known word encoding technique, word embeddings. Word embeddings are vectors\n",
    "representation of words. The main hypothesis is that words have a linear relation along with them. For example, the linear relation\n",
    "between the word `king` and `queen` is gender. So logically, if we remove the vector of `male` to `king` and add the vector\n",
    "of `female`, we should obtain the vector of `queen`. That been said; usually, those kind of representation are in dimension `300`, which\n",
    "make it impossible for humans to reason about. Still, the idea is there but in a larger dimension space.\n",
    "\n",
    "So our LSTM input and hidden state dimensions will be of the same size as the word vectors.\n",
    "This size corresponds to the word embeddings dimension, which in our case will be the\n",
    "[French pre trained](https://fasttext.cc/docs/en/crawl-vectors.html) fastText embeddings of dimension `300`.\n",
    "\n",
    "### The Pytorch Model\n",
    "\n",
    "But first, let's import all the needed packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, let's create a single (i.e. one layer) unidirectional LSTM wit `input_size` and `hidden_size` of `300`. We\n",
    "will explore later on the effect of stacking more layers and of using a bidirectional approach.\n",
    "\n",
    "> See [here](https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402) the explanation why we use the `batch_first` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dimension = 300\n",
    "num_layer = 1\n",
    "bidirectional = False\n",
    "\n",
    "lstm_network = nn.LSTM(input_size=dimension,\n",
    "                       hidden_size=dimension,\n",
    "                       num_layers=num_layer,\n",
    "                       bidirectional=bidirectional,\n",
    "                       batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected Layer\n",
    "Since the output of the LSTM network is of `300`, we will use a fully-connected layer to map this large dimension space into\n",
    "a smaller one equal to the tag space (e.g. number of tags to predict (8)).\n",
    "\n",
    "But, since we want to predict the most probable tokens, we will use the softmax function\n",
    "(see [here](https://en.wikipedia.org/wiki/Softmax_function) if softmax does not ring a bell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = dimension #the output of the LSTM\n",
    "tag_dimension = 8\n",
    "\n",
    "fully_connected_network = nn.Linear(input_dim, tag_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Constants\n",
    "\n",
    "Now, let's set our training constants. We first have the CUDA device used for training (using a CPU is way too long,\n",
    "if you don't have one, you can use a Google Colab notebook).\n",
    "\n",
    "Secondly, we set the batch size (i.e. the number of elements to see before updating the model), the learning rate for the optimizer\n",
    "and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "\n",
    "epoch_number = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set Pythons's, NumPy's and PyTorch's seeds using Poutyne function so that our training is (almost) reproducible.\n",
    "\n",
    "> See [here](https://determined.ai/blog/reproducibility-in-ml/) for more explanation of why setting seed does not guarantee reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The dataset consists of `1,010,987` complete French and English Canadian address and their associated address components tags.\n",
    "Here an example `(\"420 rue des Lilas Ouest, Qu√©bec, G1V 2V3\", [StreetNumber, StreetName, StreetName, StreetName,\n",
    "Orientation, City, PostalCode, PostalCode])`.\n",
    "\n",
    "Now let's download our dataset; for simplicity, the data was already split into an 80-20 train, valid and a `100,000` test set.\n",
    "Also, the dataset is pickled for simplicity (using a Python `list`). Here is the code to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_data(saving_dir, data_type):\n",
    "    \"\"\"\n",
    "    Function to download the dataset using data_type to specify if we want the train, valid or test.\n",
    "    \"\"\"\n",
    "\n",
    "    # hardcoded url to download the pickled dataset\n",
    "    root_url = \"https://dot-layer.github.io/blog-external-assets/train_rnn/{}.p\"\n",
    "\n",
    "    url = root_url.format(data_type)\n",
    "    r = requests.get(url)\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    open(os.path.join(saving_dir, f\"{data_type}.p\"), 'wb').write(r.content)\n",
    "\n",
    "download_data('./data/', \"train\")\n",
    "download_data('./data/', \"valid\")\n",
    "download_data('./data/', \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load in memory the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "train_data = pickle.load(open(\"./data/train.p\", \"rb\"))  # 728,789 examples\n",
    "valid_data = pickle.load(open(\"./data/valid.p\", \"rb\"))  # 182,198 examples\n",
    "test_data = pickle.load(open(\"./data/test.p\", \"rb\"))  # 100,000 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explained before, the dataset is a list of `728,789` tuples where the first element is the full address, and the second is a list of the tag (the ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (the output)\n",
    "\n",
    "![data_snapshot](data_snapshot.png)\n",
    "\n",
    "### Vectorize the Dataset\n",
    "\n",
    "Since we used word embeddings as our encoded representation of the word, we need to *convert* the address into word vector, for that, we will use a vectorizer. This embedding vectorizer will extract for every word the embedding value using the pre-trained French fastText model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We use this class so that the download templating of the fasttext\n",
    "# script be not buggy as hell in notebooks.\n",
    "class LookForProgress(TextIOBase):\n",
    "    def __init__(self, stdout):\n",
    "        self.stdout = stdout\n",
    "        self.regex = re.compile(r'([0-9]+(\\.[0-9]+)?%)', re.IGNORECASE)\n",
    "        \n",
    "    def write(self, o):\n",
    "        res = self.regex.findall(o)\n",
    "        if len(res) != 0:\n",
    "            print(f\"\\r{res[-1][0]}\", end='', file=self.stdout)\n",
    "\n",
    "class EmbeddingVectorizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Embedding vectorizer\n",
    "        \"\"\"\n",
    "        with contextlib.redirect_stdout(LookForProgress(sys.stdout)):\n",
    "            fasttext.util.download_model('fr', if_exists='ignore')\n",
    "        self.embedding_model = fasttext.load_model(\"./cc.fr.300.bin\")\n",
    "\n",
    "    def __call__(self, address):\n",
    "        \"\"\"\n",
    "        Convert address to embedding vectors\n",
    "        :param address: The address to convert\n",
    "        :return: The embeddings vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for word in address.split():\n",
    "            embeddings.append(self.embedding_model[word])\n",
    "        return embeddings\n",
    "\n",
    "embedding_vectorizer = EmbeddingVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to apply a similar approach but for the address tag (e.g. StreeNumber, StreetName).\n",
    "This vectorizer needs to convert the tag into categorical values (e.g. 0, 1 ...).\n",
    "\n",
    "For simplicity, we will use a `DatasetVectorizer` class that will apply the vectorizing process using both\n",
    "the embedding and the address vectorize process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetVectorizer:\n",
    "    def __init__(self, embedding_vectorizer):\n",
    "        self.embedding_vectorizer = embedding_vectorizer\n",
    "        self.tags_set = {\n",
    "            \"StreetNumber\": 0,\n",
    "            \"StreetName\": 1,\n",
    "            \"Unit\": 2,\n",
    "            \"Municipality\": 3,\n",
    "            \"Province\": 4,\n",
    "            \"PostalCode\": 5,\n",
    "            \"Orientation\": 6,\n",
    "            \"GeneralDelivery\": 7\n",
    "        }\n",
    "\n",
    "    def vectorize(self, data):  # We vectorize inplace\n",
    "        for idx, item in enumerate(data):\n",
    "            data[idx] = self._item_vectorizing(item)\n",
    "\n",
    "    def _item_vectorizing(self, item):\n",
    "        address = item[0]\n",
    "        address_vector = self.embedding_vectorizer(address)\n",
    "\n",
    "        tags = item[1]\n",
    "        idx_tags = self._convert_tags_to_idx(tags)\n",
    "\n",
    "        return address_vector, idx_tags\n",
    "\n",
    "    def _convert_tags_to_idx(self, tags):\n",
    "        idx_tags = []\n",
    "        for tag in tags:\n",
    "            idx_tags.append(self.tags_set[tag])\n",
    "        return idx_tags\n",
    "\n",
    "dataset_vectorizer = DatasetVectorizer(embedding_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's vectorize (in place) our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_vectorizer.vectorize(train_data)\n",
    "dataset_vectorizer.vectorize(valid_data)\n",
    "dataset_vectorizer.vectorize(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "> We use a first trick, ``padding``.\n",
    "\n",
    "Now, since all the addresses are not of the same size, it is impossible to batch them together since all tensor elements must have the same lengths. But there is a trick, padding!\n",
    "\n",
    "The idea is simple. We add *empty* tokens at the end of each sequence up to the longest one in a batch. For the word vectors, we add vectors of 0 as padding. For the tag indices, we pad with -100s. We do so because of the [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) and the accuracy metric all ignore targets with values of -100.\n",
    "\n",
    "To do this padding, we use the `collate_fn` argument of the [PyTorch `DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), and on running time, that process will be done. One thing to consider, since we pad the sequence, we need each sequence's lengths to unpad them in the forward pass. That way, we can pad and pack the sequence to minimize the training time (read [this good explanation](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) of why we pad and pack sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    The collate_fn that can add padding to the sequences so all can have\n",
    "    the same length as the longest one.\n",
    "\n",
    "    Args:\n",
    "        batch (List[List, List]): The batch data, where the first element\n",
    "        of the tuple is the word idx and the second element are the target\n",
    "        label.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (x, y). The element x is a tuple containing (1) a tensor of padded\n",
    "        word vectors and (2) their respective lengths of the sequences. The element\n",
    "        y is a tensor of padded tag indices. The word vectors are padded with vectors\n",
    "        of 0s and the tag indices are padded with -100s. Padding with -100 is done\n",
    "        because of the cross-entropy loss and the accuracy metric ignores\n",
    "        the targets with values -100.\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets us two lists of tensors and a list of integer.\n",
    "    # Each tensor in the first list is a sequence of word vectors.\n",
    "    # Each tensor in the second list is a sequence of tag indices.\n",
    "    # The list of integer consist of the lengths of the sequences in order.\n",
    "    sequences_vectors, sequences_labels, lengths = zip(*[\n",
    "        (torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n",
    "         for (seq_vectors, labels) in sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    ])\n",
    "\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    padded_sequences_vectors = pad_sequence(sequences_vectors, batch_first=True, padding_value=0)\n",
    "\n",
    "    padded_sequences_labels = pad_sequence(sequences_labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return (padded_sequences_vectors, lengths), padded_sequences_labels\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Network\n",
    "> We use a second trick, ``packing``.\n",
    "\n",
    "Since our sequences are of variable lengths and we want to be the most efficient possible by packing them, we cannot use the [PyTorch `nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) class to define our model, so we define the forward pass for it to pack and unpack the sequences (again, you can read [this good explanation](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) of why we pad and pack sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FullNetWork(nn.Module):\n",
    "    def __init__(self, lstm_network, fully_connected_network):\n",
    "        super().__init__()\n",
    "        self.hidden_state = None\n",
    "\n",
    "        self.lstm_network = lstm_network\n",
    "        self.fully_connected_network = fully_connected_network\n",
    "\n",
    "    def forward(self, padded_sequences_vectors, lengths):\n",
    "        \"\"\"\n",
    "            Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "        total_length = padded_sequences_vectors.shape[1]\n",
    "        pack_padded_sequences_vectors = pack_padded_sequence(padded_sequences_vectors, lengths, batch_first=True)\n",
    "\n",
    "        lstm_out, self.hidden_state = self.lstm_network(pack_padded_sequences_vectors)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True, total_length=total_length)\n",
    "\n",
    "        tag_space = self.fully_connected_network(lstm_out)\n",
    "        return tag_space.transpose(-1, 1) # we need to transpose since it's a sequence\n",
    "\n",
    "full_network = FullNetWork(lstm_network, fully_connected_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So we have created an LSTM network (`lstm_network`), a fully connected network (`fully_connected_network`), those two\n",
    "components are used in the full network. This full network used padded, packed sequences (defined in the forward pass),\n",
    "so we created the `pad_collate_fn` function to process the needed work. The DataLoader will conduct that process. Finally,\n",
    "when we load the data, this will be done using the vectorizer, so the address will be represented using word embeddings.\n",
    "Also, the address components will be converted into categorical value (from 0 to 7).\n",
    "\n",
    "\n",
    "## The Training\n",
    "\n",
    "Now that we have all the components for the network let's define our Stochastic Gradient Descent\n",
    "([SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(full_network.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poutyne Experiment\n",
    "> Disclaimer that I'm a dev on Poutyne, so I will present code using this framework. See the project [here](https://poutyne.org/).\n",
    "\n",
    "Let's create our experiment using Poutyne for automatically logging in the project root directory (`./`). We will also set\n",
    "the loss function and a batch metric (accuracy) to monitor the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp = Experiment(\"./\", full_network, device=cuda_device, optimizer=optimizer,\n",
    "                 loss_function=cross_entropy, batch_metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our experiment, we can now launch the training as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.train(train_loader, valid_generator=valid_loader, epochs=epoch_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take around 6 minutes per epochs, so around an hour for the complete training.\n",
    "\n",
    "### Results\n",
    "The next figure presents the loss and the accuracy during training (blue) and during validation (orange).\n",
    "After 10 epochs, we obtain a validation loss and accuracy of `0.01981` and `99.54701` respectively, which is pretty\n",
    "good for a first model. Also, we can see that our model did not seem to have overfitted.\n",
    "\n",
    "![loss_acc](graph/training_graph.png)\n",
    "\n",
    "## Bigger model\n",
    "\n",
    "It seems like our model performed pretty well, but just for fun, let's unleash the full potential of LSTM using a\n",
    "bidirectional approach (bi-LSTM). What it means is instead of _simply_ using a seeing the sequence from the start to the end, we\n",
    "also train the model to see the sequence from the end to the start. It's important to state that the two directions are\n",
    "not shared, meaning that we _see_ the sequence in one direction at the time but we gather both the information for the\n",
    "fully connected layer. That way, our model can get insight from both direction.\n",
    "\n",
    "Instead of using only one layer, let's use an LSTM, which means that we use two layers of hidden state.\n",
    "\n",
    "So, let's create the new LSTM and fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dimension = 300\n",
    "num_layer = 2\n",
    "bidirectional = True\n",
    "\n",
    "lstm_network = nn.LSTM(input_size=dimension,\n",
    "                       hidden_size=dimension,\n",
    "                       num_layers=num_layer,\n",
    "                       bidirectional=bidirectional,\n",
    "                       batch_first=True)\n",
    "\n",
    "input_dim = dimension * 2 #since bidirectional\n",
    "\n",
    "fully_connected_network = nn.Linear(input_dim, tag_dimension)\n",
    "\n",
    "full_network_bi_lstm = FullNetWork(lstm_network, fully_connected_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_bi_lstm = Experiment(\"./\", full_network_bi_lstm, device=cuda_device, optimizer=optimizer,\n",
    "                 loss_function=cross_entropy, batch_metrics=[\"acc\"])\n",
    "exp_bi_lstm.train(train_loader, valid_generator=valid_loader, epochs=epoch_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Here are the last epoch validation results of the larger model. On the validation dataset,\n",
    "we can see that we obtain a marginal gain of around `0.3` for the accuracy over the previous one. Not much of an improvement.\n",
    "\n",
    "|   Model  | Bi-LSTM two layers |\n",
    "|:--------:|:------------------:|\n",
    "|   Loss   |    0.0050          |\n",
    "| Accuracy |    99.8594         |\n",
    "\n",
    "But now that we have our two trained models, let's use the test set as a final an **unique** steps of evaluating the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.test(test_loader)\n",
    "exp_bi_lstm.test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next table presents the results of the Bi-LSTM with two layers and the previous model (LSTM with one layer).\n",
    "\n",
    "|   Model  | LSTM one layer | Bi-LSTM two layers |\n",
    "|:--------:|:--------------:|:------------------:|\n",
    "|   Loss   |     0.0152     |    **0.0050**      |\n",
    "| Accuracy |     99.5758    |    **99.8550**     |\n",
    "\n",
    "We can see similar results as the validation one for both the model. Also, we still see a little improvement of the accuracy and the loss for the larger model. But when considering that we only improved by around 0.3, one can\n",
    "argue that it's only a matter of the seed. To test our approach's robustness, we could use retrain multiple times\n",
    "our training step but using a different seed every time. Using those trained models, we can report the mean and one standard variation of the metrics instead of a single training. But instead of doing that, let's try something else.\n",
    "\n",
    "#### Zero Shot Evaluation\n",
    "Since we have to our disposition other country address, let's see if our model has really learned a typical address sequence\n",
    "or he has simply learned all the cases (know as memorization).\n",
    "\n",
    "We will test three different cases\n",
    "\n",
    "   - the first one we will test if our model performs well on countries using the exact same\n",
    "    address structure as our training dataset: United-States of America (US) and United-Kingdom (UK)\n",
    "   - the second one we will test if our model performs well on a country using the exact same address structure **but**\n",
    "    using a totally different language: Russia (RU)\n",
    "   - the last one will test if our model performs well on a country using a different address structure **and** a different language: Mexico (MX).\n",
    "\n",
    "For each, we will have a total dataset of `100,000` examples, and we will use the last and best epoch results (the 10th).\n",
    "Also, we will use the same pre-processing steps (i.e. data vectorization, the same pad collate function), but we will only apply\n",
    "a test phase.\n",
    "\n",
    "But first, let's download and vectorize all the needed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "download_data('./data/', \"us\")\n",
    "download_data('./data/', \"gb\")\n",
    "download_data('./data/', \"ru\")\n",
    "download_data('./data/', \"mx\")\n",
    "\n",
    "us_data = pickle.load(open(\"./data/us.p\", \"rb\"))  # 100,000 examples\n",
    "gb_data = pickle.load(open(\"./data/gb.p\", \"rb\"))  # 100,000 examples\n",
    "ru_data = pickle.load(open(\"./data/ru.p\", \"rb\"))  # 100,000 examples\n",
    "mx_data = pickle.load(open(\"./data/mx.p\", \"rb\"))  # 100,000 examples\n",
    "\n",
    "dataset_vectorizer.vectorize(us_data)\n",
    "dataset_vectorizer.vectorize(gb_data)\n",
    "dataset_vectorizer.vectorize(ru_data)\n",
    "dataset_vectorizer.vectorize(mx_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Test\n",
    "\n",
    "Now let's test for the United-States of America and United-Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "us_loader = DataLoader(us_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(us_loader)\n",
    "exp_bi_lstm.test(us_loader)\n",
    "\n",
    "gb_loader = DataLoader(gb_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(gb_loader)\n",
    "exp_bi_lstm.test(gb_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next table presents the results of both the model for both the country. We first see that we obtain\n",
    "better results for the two countries using the BLSTM (around 8% better). It's interesting to see that even if the structure is similar. Also, the\n",
    "presence of the same language as in the training dataset (i.e. English), we obtain poorer results than before. That situation\n",
    "is mostly due to the postal code format is not similar. For the US, it is 5 digits, and for the UK it is similar to Canada, but it is not always a letter followed by a number, and it is not always 6 characters. It's *normal* for a model to\n",
    "have difficulty is that kind of new pattern, but he has still achieved good results.\n",
    "\n",
    "| Model (Country) | LSTM one layer | Bi-LSTM two layers |\n",
    "|:---------------:|:--------------:|:------------------:|\n",
    "|    Loss (US)    |     0.6176     |       **0.3078**   |\n",
    "|  Accuracy (US)  |     84.7396    |       **91.8220**  |\n",
    "|    Loss (UK)    |     0.4368     |       **0.1571**   |\n",
    "|  Accuracy (UK)  |     86.2543    |       **95.6840**  |\n",
    "\n",
    "\n",
    "##### The Second and Third Test\n",
    "\n",
    "Now let's tests for Russia and Mexico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ru_loader = DataLoader(ru_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(ru_loader)\n",
    "exp_bi_lstm.test(ru_loader)\n",
    "\n",
    "mx_loader = DataLoader(mx_data, batch_size=batch_size, collate_fn=pad_collate_fn)\n",
    "exp.test(mx_loader)\n",
    "exp_bi_lstm.test(mx_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next table presents the results of both the model for both the country tested. We first see that the first test\n",
    "(RU) gives poorer results than Mexico, even if the second one is a different structure and language.\n",
    "This situation could be explained by both languages' language roots; Spanish is closer to French than Russia.\n",
    "But an interesting thing is that even in a *difficult* annotation context, both the model perform relatively well.\n",
    "It means that our models seem to have really learned the *logic* of an address sequence. It could also mean that if\n",
    "we train our model longer, maybe we could improve our results. But, other improvements will be discussed in the next summary section.\n",
    "\n",
    "| Model (Country) | LSTM one layer | Bi-LSTM two layers |\n",
    "|:---------------:|:--------------:|:------------------:|\n",
    "|    Loss (RU)    |   **2.5181**   |       4.6118       |\n",
    "|  Accuracy (RU)  |   **48.9820**  |       47.3185      |\n",
    "|    Loss (MX)    |     2.6786     |     **1.7147**     |\n",
    "|  Accuracy (MX)  |     50.2013    |     **63.5317**    |\n",
    "\n",
    "### Summary\n",
    "In summary, we found that using a Bi-LSTM with two layers seems to perform better on countries' addresses not seen during training. Still, the results are not as good as those of Canada (training dataset). A solution to this problem could be to train a model using all the\n",
    "possible data in the world. This approach was used by [Libpostal](https://github.com/openvenues/libpostal) which trained a\n",
    "CRF over an impressive near `100` million address (yes, **100 million**). The data is publicly available if you want to explore this avenue next.\n",
    "\n",
    "We also explored that the language has a negative impact on the results since we use monolingual word embeddings (i.e. French),\n",
    "which is *normal* considering that they were trained for a specific language. A possible solution to that problem is the use of subword embedding composed of sub-division of a word instead of the complete one. For example, a two characters window embeddings of `H1A1` would be the aggregate embeddings of the subword `H1`, `1A` and `A1`.\n",
    "\n",
    "> Alert of self-promotion of my work here\n",
    "I've personally explored this avenue in an article about using [subword embedding for address parsing](https://arxiv.org/abs/2006.16152).\n",
    "\n",
    "That being said, our model still performed well on the Canadian dataset, and one can simply train simpler LSTM model using\n",
    "country data to obtain the best results possible with the simpler model as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}