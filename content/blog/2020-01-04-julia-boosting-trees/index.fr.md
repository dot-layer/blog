---
title: "Boosting Trees avec Julia"
author: "Jeremie Desgagne-Bouchard"
date: "2020-02-04"
slug: "julia-boosting-trees"
type: "post"
tags: ["Machine Learning", "Julia"]
featured: "julia-plot.png"
featuredpath: "img/headers/"
description: "D√©veloppement d'algorithmes from scratch"
---


> Cet article a pour but d'exposer les principes cl√©s permettant une implantation haute performance du _gradient boosting trees_ en Julia, un langage r√©conciliant l'expressivit√© et la productivit√© qu'on retrouve en Python et en R et la performance de langages compil√©s comme le C et C++. 

Bien que les approches par r√©seaux de neurones accaparent une bonne partie de l'attention, l'importance des algorithmes reposant sur des arbres de d√©cision ne peut √™tre n√©glig√©e. Ils continuent de se d√©marquer comme offrant la meilleure performance pr√©dictive dans de nombreuses situations, particuli√®rement lorsqu'il s'agit de probl√®mes de r√©gression ou de classification impliquant des donn√©es tabulaires.

Parmi les plus c√©l√®bres repr√©sentants de cette famille d'algorithmes, on compte [XGBoost](https://xgboost.readthedocs.io/en/latest/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/) et [CatBoost](https://catboost.ai/). Si ces derni√®res implantations sont relativement r√©centes (2014, 2016 et 2017), l'id√©e avait √©t√© d√©velopp√©e depuis d√©j√† quelques ann√©es puisqu'on la retrouve d√®s 2001 dans le d√©sormais classique [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/). 

Il serait hasardeux d'apporter un diagnostic d√©finitif sur ce qui a conduit √† l'explosion de popularit√© de l'algorithme. L'int√©r√™t pour l'apprentissage machine et le d√©veloppement d'une approche comp√©titive √† la mod√©lisation via des plateformes comme [Kaggle](https://www.kaggle.com/) n'y sont sans doute pas √©trangers. Un atout du gradient boosting est √©galement sa rapidit√©: XGBoost apportait √† sa sortie une r√©duction du temps d'entra√Ænement de l'ordre de 10X par rapport aux implantations en R et Python existantes. 

Dans un contexte d'utilisation commerciale, les enjeux de performance deviennent rapidement significatifs compte tenu des volumes de donn√©es impliqu√©s. Le souci qu'on y accorde au sein de la nouvelle g√©n√©ration d'algorithmes n'est sans doute pas √©tranger √† ces imp√©ratifs commerciaux. Aussi, lorsqu'il est question de performance, le coeur d'un algorithme est typiquement d√©velopp√© dans un langage compil√© (C/C++), bien que l'utilisateur interagit le plus souvent avec celui-ci au travers d'interfaces en Python ou R qui facilitent le d√©veloppement exp√©rimental.  

Une facette int√©ressante du langage Julia est qu'il permet de briser cette barri√®re des 2 langages. La figure ci-dessous montre que contrairement √† XGBoost, l'int√©gralit√© de l'implantation [Julia du gradient boosting](https://github.com/Evovest/EvoTrees.jl) est cod√©e... en Julia! 

![](xgboost_github.PNG)

![](julia_github.PNG)

On verra maintenant plus en d√©tail comment il est possible d'implanter des algorithmes "from scratch" dans un langage convivial tout en obtenant des vitesses d'ex√©cution comp√©titives avec les solutions les plus performantes sur le march√©. 

## Mise en contexte

Afin de rendre plus tangibles les d√©tails de l'implantation du gradient boosting en Julia, un probl√®me de r√©gression avec 2 variables continues servira d'exemple. 

La variable r√©ponse est d√©pendante des variables `var1` et `var2`. L'effet est sinuso√Ødal en `var1` et croissant en `var2`.

![](data_3D.png)

![](raw_one_ways.png)


## Introduction √† l'algorithme

L'entra√Ænement d'un gradient boosting trees (GBT) peut √™tre d√©crit sommairement de la mani√®re suivante: 

0. D√©finir une pr√©diction de base pour chacune des observations. Ex: pred = 0.0
1. Construire un arbre de d√©cision, _A1_ expliquant la diff√©rence entre les pr√©dictions et les valeurs observ√©es.  
2. Mettre √† jour les pr√©dictions en ajoutant les pr√©dictions de l'arbre _A1_ aux pr√©dictions actuelles: pred = pred + predict(_A1_)
3. R√©p√©ter 1. et 2. pour un nombre N d'arbres.

Dans un sc√©nario o√π le nombre d'it√©rations serait de 4, le mod√®le entra√Æn√© pourrait se visualiser de la fa√ßon suivante: 

![](tree_group.png)

Pour obtenir une pr√©diction, il suffit d'additionner la pr√©diction obtenue √† chacun des 4 arbres. 

Comme on peut le constater, un mod√®le GBT consiste en une collection d'arbres de d√©cision. Quelques subtilit√©s sont n√©anmoins introduites en pratique, par exemple le r√©√©chantillonnage des observations et des variables explicatives √† chacune des it√©rations. Reste qu'une fois qu'on a √©tabli comment construire un arbre de d√©cision, l'essentiel du travail est accompli. √Ä noter que la m√™me logique s'appliquerait si on construisait un RandomForest: il ne suffirait encore l√† que de savoir construire un arbre de d√©cision, le reste n'√©tant qu'une variation de l'algorithme pr√©sent√© plus haut. 

En Julia, on peut d√©finir la structure du mod√®le de la mani√®re suivante, o√π un GBTree est compos√© d'un vecteur de Tree: 

```julia
struct GBTree
    trees::Vector{Tree}
    params
    metric
end
```

En son coeur Julia supporte des repr√©sentations multi-dimensionnelles via des `Array{T,N}`. Un vecteur `Vector{T}` ou une matrice `Matrix{T}` ne sont que des cas particuliers des `Array{T,N}`, o√π `N` = 1 et 2 respectivement. L'√©l√©ment `T` r√©f√®re au type. Par exemple, un vecteur peut √™tre d√©fini par: `[1.1, 2.2]`. La nature de cet objet serait `Vector{Float64}`. En Julia, la repr√©sentation multi-dimensionnelle ne se limite pas aux nombres conventionnels comme les Float ou les Integer, √ßa peut √™tre n'importe quel type d‚Äôobjet. Par exemple, on pourrait parfaitement avoir une matrice dont les √©l√©ments sont des DataFrames (mais le produit matriciel de ces objets resterait √† d√©finir!). Dans le cas du GBT, le mod√®le est ainsi constitu√© d'un `Vector{Tree}`. 

## D√©finition d'un arbre

Tel que montr√© plus haut, un arbre de d√©cision se compose d'une s√©rie de noeuds comportant chacun une d√©cision binaire. Par exemple, dans l'arbre ci-dessous, on commence par √©tablir, pour chaque observation, si la variable 1 est plus petite que 0.996. Si oui, on va dans le segment de gauche et la d√©cision suivante est si la variable 2 est plus petite que 1.12. On arrive ensuite √† un noeud terminal qui indique la pr√©diction √† associer √† l'observation. 

![](tree_1.png)

Une structure r√©cursive peut √™tre une repr√©sentation intuitive pour un arbre qui serait alors d√©fini comme un noeud contenant le crit√®re de d√©cision ainsi que 2 noeuds d√©pendants ("child nodes") selon que la condition soit respect√©e ou non. Il est √©galement possible de repr√©senter un arbre par un simple vecteur de noeuds: 

```julia
struct Tree{L, T<:AbstractFloat, S<:Int}
    nodes::Vector{TreeNode}
end
```

```julia
struct TreeNode{L, T<:AbstractFloat, S<:Int, B<:Bool}
    left::S
    right::S
    feat::S
    cond::T
    pred::SVector{L,T}
    split::B
end
```

Comme on peut le voir dans la structure `TreeNode`, chaque noeud d√©finit sur quelle variable la d√©cision est prise (`feat`) ainsi que la condition √† appliquer (`cond`). S'il s'agit d'un noeud terminal, il contiendra la pr√©diction (`pred`) et l'indicateur `split` sera √† `false`. 

Une fois les structures √©tablies, il ne reste plus qu'√† identifier les valeurs qu'elles doivent prendre. 

## Construction d'un arbre

Pour construire un arbre, il s'agit d'√©valuer pour chaque variable la condition apportant la plus grande r√©duction de la fonction de perte (la somme des erreurs au carr√© par exemple). C'est l√† que l'essentiel de la charge de calcul se trouve et que certains choix de design permettront d'atteindre des performances optimales. Ensuite, la variable dont la condition optimale apporte le plus grand gain sera retenue pour d√©finir la condition du noeud. 

Pour chaque noeud, l'algorithme s'exerce d'une perspective univari√©e. Il s'agit l√† d'une propri√©t√© se pr√™tant √† une optimisation. Puisque l'√©valuation de la meilleure condition se fait de fa√ßon ind√©pendante pour chaque variable, cette recherche peut ais√©ment √™tre parall√©lis√©e.

Julia supporte plusieurs saveurs de parall√©lisme. Dans le cas de la recherche de variables, tous les coeurs du processeur peuvent √™tre mis √† profit simplement en utilisant la macro `@threads` incluse dans les fonctionnalit√©s de base du langage.  

```julia
@threads for j in cols
  find_best_split!(...)
end
```

Une fa√ßon brute de chercher le meilleur bris est de mettre en ordre les observations selon une variable donn√©e. Une fois les observations en ordre, on peut consid√©rer pour chacune des valeurs uniques prises par cette variable quel serait le gain si la condition s'exer√ßait sur cette valeur. 

Une telle approche fonctionne, mais est sujette √† quelques inconv√©nients. D'abord, ordonner une variable est une op√©ration co√ªteuse, particuli√®rement si on consid√®re que l'op√©ration doit √™tre r√©p√©t√©e pour plusieurs variables, pour chacun des noeuds et pour chaque arbre. √âgalement, si le nombre de valeurs uniques prises par une variable est tr√®s √©lev√©, √ßa implique d'√©valuer le gain √† un tr√®s grand nombre de reprises. 

La m√©thode de l'histogramme permet de contourner ces obstacles. L'id√©e est de discr√©tiser chaque variable en associant chaque observation √† un groupe, par exemple le quantile. En utilisant un entier entre 0 et 255 comme identifiant de ces groupes, la matrice de donn√©es est encod√©e dans un format `UInt8`, lequel accapare 8 fois moins de m√©moire qu'un format `Float64` (un _numeric_ en R). 

Avant la construction des arbres, la librairie EvoTrees effectue cette discr√©tisation en trouvant d'abord les quantiles pour chacune des variables (get_edges), puis en cr√©ant une matrice de `UInt8` pour encoder les donn√©es d'entra√Ænement.

```julia
edges = EvoTrees.get_edges(X_train, params.nbins)
X_bin = EvoTrees.binarize(X_train, edges)
```

En choisissant le nombre de groupe (nbins) comme √©tant 16, le probl√®me √† r√©soudre prend la forme suivante d'un point de vue univari√©: 

![](bin_one_ways.png)

Sous cette formulation, le nombre de conditions √† √©valuer se limite d√©sormais √† 15 (ou plus g√©n√©ralement, nbins-1). 

Il reste enfin √† d√©finir le gain associ√© √† chacun des bris. Une force du gradient boosting est sa flexibilit√©. Sous l'implantation introduite par XGboost, il suffit de d√©finir une fonction de perte qui soit convexe. Par exemple, avec une r√©gression des moindres carr√©s, la perte est d√©finie par $(y - pred)^2$. Cette perte a une forme parabolique et son minimum est bien entendu lorsque la valeur pr√©dite √©gale la valeur observ√©e. La notion critique √† remarquer est qu'en ne connaissant que les d√©riv√©es premi√®res et secondes de la perte par rapport √† la pr√©diction, il est possible de d√©terminer non seulement quelle serait la pr√©diction optimale, mais √©galement le gain r√©alis√© et ce, peu importe la fonction de perte gr√¢ce √† une approximation de second degr√©: 

```julia
pred = -params.Œ∑ .* node.‚àëŒ¥ ./ (node.‚àëŒ¥¬≤ .+ params.Œª .* node.‚àëùë§)
gain = sum((‚àëŒ¥ .^ 2 ./ (‚àëŒ¥¬≤ .+ Œª .* ‚àëùë§)) ./ 2
```

O√π Œ∑ = vitesse d'apprentissage, ‚àëŒ¥, ‚àëŒ¥¬≤ = somme des d√©riv√©es premi√®res et secondes, ‚àëw = somme des poids et Œª = facteur de r√©gularisation. 

Pour la premi√®re des 15 conditions possibles, l'arbre distinguerait les donn√©es en deux groupes (gauche et droite) de la mani√®re suivante: 

![](first_split.png)

Le gain se d√©finit comme la r√©duction de la perte qu'apporterait une modification √† la pr√©diction. Il est calcul√© s√©par√©ment pour les groupes de gauche et de droite puis additionn√©. La perte associ√©e √† diff√©rentes modifications de la pr√©diction pour le groupe de gauche est la suivante:

![](left_parabole.png)

La perte minimale est atteinte lorsque la pr√©diction est de -1.11, ce qui correspond √† la moyenne des r√©sidus pour le groupe 1. La perte sur l'ensemble des donn√©es est √©galement calcul√©e. Effectuer un bris dans l'arbre devra apporter un gain par rapport √† cette valeur de r√©f√©rence. 

Au terme du processus d'entra√Ænement, le mod√®le prend la forme suivante:

![](julia_gbtree_struct.png)

## √âvaluation de la performance

Afin d'√©valuer si l'implantation de l'algorithme est comp√©titive, une comparaison du temps d'entra√Ænement par rapport √† XGBoost pour 100 it√©rations sur des donn√©es g√©n√©r√©es al√©atoirement est conduite:  

| Dimensions / Algo | XGBoost Exact | XGBoost Hist | EvoTrees |   |
|-------------------|:-------------:|:------------:|:--------:|---|
| 10K x 100         |     1.18s     |     2.15s    |   0.52s  |   |
| 100K x 100        |     9.39s     |     4.25s    |   2.02s  |   |
| 1M X 100          |     146.5s    |     20.2s    |   22.5   |   |

Il en ressort que la m√©thode par histogramme est critique pour obtenir de bonnes performances sur des donn√©es volumineuses. Aussi, au-del√† du million d'observations, XGBoost reprend l'avantage sur EvoTrees. 

EvoTrees supporte par ailleurs quelques fonctions de pertes qu'on ne retrouve pas dans XGBoost, dont la r√©gression par quantile ainsi que la r√©gression gaussienne (estimation simultan√©e des param√®tres $\mu$ et $\sigma$ de la distribution). 

## D√©veloppements futurs

Une piste de d√©veloppement seest de consid√©reriff√©rents modes de parall√©lisme avec des donn√©es plus volumineuses afin de combler l'√©cart de performance avec des donn√©es de > 1 M d'observations, par exemple en parall√©lisant la construction d'histogrammes √† l'int√©rieur d'une m√™me variable. √âgalement, fid√®le √† l'esprit de r√©solution du probl√®me des deux langages, Julia offre des fonctionnalit√©s prometteuses pour le d√©veloppement d'algorithmes sur [GPU](https://juliacomputing.com/domains/gpus.html). Supporter la construction d'histogrammes en CUDA pourrait ainsi √™tre la meilleure r√©ponse pour le traitement de donn√©es tr√®s volumineuses.  

## Conclusion

Au-del√† de la librairie EvoTrees, Julia offre un environnement de choix pour toute t√¢che exigeante en calculs. Le projet [MLJ](https://github.com/alan-turing-institute/MLJ.jl) auquel Evotrees est int√©gr√© offre un bon point d'entr√©e pour tout projet de mod√©lisation traditionnel. Plus significatif encore est de r√©aliser que de nombreuses routines algorithmiques peuvent √™tre implant√©es relativement ais√©ment, permettant de se lib√©rer des lourdeurs communes aux librairies cl√©s en main pour se concentrer sur les caract√©ristiques uniques du probl√®me √† r√©soudre. La l√©g√®ret√© de l'outil d'apprentissage profond [Flux](https://fluxml.ai/) en est un exemple √©loquent. 
